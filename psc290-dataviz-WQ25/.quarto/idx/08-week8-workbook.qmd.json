{"title":"Week 8 (Workbook) - Polishing Visualizations","markdown":{"yaml":{"title":"Week 8 (Workbook) - Polishing Visualizations","author":"Emorie D Beck","format":{"html":{"code-tools":true,"code-copy":true,"code-line-numbers":true,"code-link":true,"theme":"united","highlight-style":"tango","df-print":"paged","code-fold":"show","toc":true,"toc-float":true,"self-contained":true,"footer":"PSC 290 - Data Visualization","logo":"https://github.com/emoriebeck/psc290-data-viz-2022/raw/main/01-week1-intro/02-code/02-images/ucdavis_logo_blue.png"}},"editor_options":{"chunk_output_type":"console"}},"headingText":"Polishing & Hacking Your Visualizations","containsRefs":false,"markdown":"\n\n```{r, echo = F}\nknitr::opts_chunk$set(echo = TRUE, warning = F, message = F, error = F, out.width = \"90%\", fig.align=\"center\")\noptions(knitr.kable.NA = '')\n```\n\n\n## Packages\n\n```{r, echo = T}\n#| code-line-numbers: \"11-13\"\nlibrary(RColorBrewer)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(plyr)\nlibrary(broom)\nlibrary(modelr)\nlibrary(lme4)\nlibrary(broom.mixed)\nlibrary(tidyverse)\nlibrary(ggdist)\nlibrary(patchwork)\nlibrary(cowplot)\nlibrary(DiagrammeR)\nlibrary(wordcloud)\nlibrary(tidytext)\nlibrary(ggExtra)\nlibrary(distributional)\nlibrary(gganimate)\n```\n\n## Custom Theme:\n\n```{r}\nmy_theme <- function(){\n  theme_classic() + \n  theme(\n    legend.position = \"bottom\"\n    , legend.title = element_text(face = \"bold\", size = rel(1))\n    , legend.text = element_text(face = \"italic\", size = rel(1))\n    , axis.text = element_text(face = \"bold\", size = rel(1.1), color = \"black\")\n    , axis.title = element_text(face = \"bold\", size = rel(1.2))\n    , plot.title = element_text(face = \"bold\", size = rel(1.2), hjust = .5)\n    , plot.subtitle = element_text(face = \"italic\", size = rel(1.2), hjust = .5)\n    , strip.text = element_text(face = \"bold\", size = rel(1.1), color = \"white\")\n    , strip.background = element_rect(fill = \"black\")\n    )\n}\n```\n\n# Diagrams\n\n-   In research, we often need to make diagrams all points in our research, from\n    -   conceptualizing study flow\n    -   mapping measures\n    -   mapping verbal models\n    -   SEM models\n    -   and more\n\n## `DiagrammeR`\n\n-   [`DiagrammeR`](http://rich-iannone.github.io/DiagrammeR/docs.html) is a unique interface because it brings together multiple ways of building diagrams in R and tries ot unite them with consistent syntax\\\n-   We could spend a whole *course*, not just part of one *class* parsing through the `DiagrammeR` package, so I'm going to make a strong assumption based on my knowledge of your ongoing interests and research:\n    -   SEM plots\n    -   network visualizations\n    -   combinations of both\n-   Let's just jump in!\n\n```{r, eval = F}\n[strict] (graph | digraph) [ID] '{' stmt_list '}'\n```\n\n1.  `strict` basically determines whether we can multiple nodes going into / out of a node\n2.  We have to tell Graphviz whether want a directed `[digraph]` or undirected `[graph]` graph.\n3.  `[ID]` is what you want to name your graph object\n4.  `'{' stmt_list '}'` is where you specify the nodes and edges the graph (more on this next)\n\n```{r}\ngrViz(\"\ndigraph ex1 {\n\n  # a 'graph' statement\n  graph [overlap = true, fontsize = 10]\n\n  # several 'node' statements\n  node [shape = box,\n        fontname = Helvetica]\n  A; B; C; D; E; F\n}\"\n)\n```\n\n-   `digraph` says we want the graph to be directed\n-   `graph` lets us control elements of the graph in the `[]`\n    -   `overlap = true` means nodes can overlap\n-   `node` means we're about to specify some nodes (and their properties in `[]`)\n\n### Nodes\n\nWe can control lots of properties of nodes (either as groups or individually):\n\n:::::: columns\n::: {.column width=\"34%\"}\n-   color\n-   fillcolor\n-   fontcolor\n-   alpha\n-   shape\n-   style (like linestyle)\n-   sides\n:::\n\n::: {.column width=\"33%\"}\n-   peripheries\n-   fixedsize\n-   height\n-   width\n-   distortion\n-   penwidth\n:::\n\n::: {.column width=\"33%\"}\n-   x\n-   y\n-   tooltip\n-   fontname\n-   fontsize\n-   icon\n:::\n::::::\n\n-   See [documentation](http://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html#node-shapes) for more info!\n\n### Edges\n\nBut we also want to add edges\n\n```{r}\ngrViz(\"\ndigraph ex1 {\n\n  # a 'graph' statement\n  graph [overlap = true, fontsize = 10]\n\n  # several 'node' statements\n  node [shape = box,\n        fontname = Helvetica]\n  A; B; C; D; E; F\n  \n  # several 'edge' statements\n  A->B B->C C->D D->E E->F\n}\"\n)\n```\n\n-   `->` indicates directed edges\n-   `--` indicates undirected edges\n-   `A->{B,C}` is the same as `A->B A->C`\n\nEdge properties can be defined like node properties:\n\n::::: columns\n::: column\n-   `arrowsize`\n-   `arrowhead`\n-   `arrowtail`\n-   `dir`\n-   `color`\n-   `alpha`\n-   `headport`\n:::\n\n::: column\n-   `tailport`\n-   `fontname`\n-   `fontsize`\n-   `fontcolor`\n-   `penwidth`\n-   `menlin`\n-   `tooltip`\n:::\n:::::\n\n-   See [documentation](http://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html#arrow-shapes) for more information on these!\n\n### Example: Big Five\n\n-   Let's do the Big Five because why not?\n-   But they aren't orthogonal, so we need to let the factors correlate.\n\n```{r}\ngrViz(\"\ndigraph b5 {\n\n  # a 'graph' statement\n  graph [overlap = true, fontsize = 10]\n\n  # def latent Big Five\n  node [shape = circle]\n  E; A; C; N; O\n  \n  # def observed indicators\n  node [shape = square]\n  e1; e2; e3\n  a1; a2; a3\n  c1; c2; c3\n  n1; n2; n3\n  o1; o2; o3\n  \n  # several 'edge' statements\n  E->{e1,e2,e3}\n  A->{a1,a2,a3}\n  C->{c1,c2,c3}\n  N->{n1,n2,n3}\n  O->{o1,o2,o3}\n}\"\n)\n```\n\n-   But they aren't orthogonal, so we need to let the factors correlate.\n-   Mess\n\n```{r}\ngrViz(\"\ndigraph b5 {\n\n  # a 'graph' statement\n  graph [overlap = true, fontsize = 10]\n\n  # def latent Big Five\n  node [shape = circle]\n  E; A; C; N; O\n  \n  # def observed indicators\n  node [shape = square]\n  e1; e2; e3\n  a1; a2; a3\n  c1; c2; c3\n  n1; n2; n3\n  o1; o2; o3\n  \n  # several 'edge' statements\n  E->{e1,e2,e3}\n  A->{a1,a2,a3}\n  C->{c1,c2,c3}\n  N->{n1,n2,n3}\n  O->{o1,o2,o3}\n  \n  E->{A,C,N,O} [dir = both]\n  A->{C,N,O} [dir = both]\n  C->{N,O} [dir = both]\n  N->{O} [dir = both]\n}\"\n)\n```\n\nLet's change the layout to `neato` because that's kind of a mess!\n\n```{r}\ngrViz(\"\ndigraph b5 {\n\n  # a 'graph' statement\n  graph [overlap = true, fontsize = 10, layout = neato]\n\n  # def latent Big Five\n  node [shape = circle]\n  E; A; C; N; O\n  \n  # def observed indicators\n  node [shape = square,\n        fixedsize = true,\n        width = 0.25]\n  e1; e2; e3\n  a1; a2; a3\n  c1; c2; c3\n  n1; n2; n3\n  o1; o2; o3\n  \n  # several 'edge' statements\n  E->{e1,e2,e3}\n  A->{a1,a2,a3}\n  C->{c1,c2,c3}\n  N->{n1,n2,n3}\n  O->{o1,o2,o3}\n  \n  E->{A,C,N,O} [dir = both]\n  A->{C,N,O} [dir = both]\n  C->{N,O} [dir = both]\n  N->{O} [dir = both]\n}\"\n)\n```\n\n-   That was all very `lavaan`, wasn't it?\n-   Well, sometimes we want to create diagrams using code or pipelines, which isn't easy or intuitive using the syntax we've been using\n-   So instead, we can create the same visualizations using `create_graph()` and accompanying functions\n-   Unfortunately, we don't have time for that today, but there's a great tutorial [online](http://rich-iannone.github.io/DiagrammeR/graph_creation.html)\n\n# Basic Text Visualization\n\n-   In some ways, the hardest part of text visualization is *getting* the text into `R`.\n-   Once text is in `R`, there are lots of great tools for tokenizing, basic sentiment analysis, and more\n-   We'll be relying on [*Tidy Text Analysis in R*](https://www.tidytextmining.com/index.html)\n-   Today, we'll use some data from an ongoing project of mine that applies NLP to *Letters from Jenny* (Anonymous, 1942), which were published in the *Journal of Abnormal and Social Psychology*\\\n-   The PDF's have been converted to a .txt file\n\n```{r}\ntext_df <- read.table(\"https://github.com/emoriebeck/psc290-data-viz-2022/raw/main/08-week8-polishing/01-data/part2_pymupdf.txt\", sep = \"\\n\") %>%\n  setNames(\"text\") %>%\n  mutate(line = 1:n()) %>%\n  as_tibble() %>%\n  mutate(text = str_remove_all(text, \"[0-9]\"))\ntext_df$text[1:10]\n```\n\n## Tokens\n\n-   The first step with text data is to clean and tokenize it.\n-   Cleaning basically means makoing sure that everything parsed correctly\\\n-   Tokenizing means that we break the text down into tokens that we can then analyze\n-   We tokenize for lots of reasons. It let's us:\n    -   Remove filler words\n    -   Group words in different forms, tenses\n    -   Get rid of punctuation, etc.\n    -   And more\n\n> A token is a meaningful unit of text, most often a word, that we are interested in using for further analysis, and tokenization is the process of splitting text into tokens (Silge & Robinson, *Tidy Text Mining in R*)\n\n```{r}\ntidy_text <- text_df %>%\n  unnest_tokens(word, text)\ntidy_text\n```\n\nNow, let's remove stop words (articles, etc.) that we don't want to analyze:\n\n```{r}\ndata(stop_words)\n\ntidy_text <- tidy_text %>%\n  anti_join(stop_words)\n```\n\nLet's count the frequency of words:\n\n```{r}\ntidy_text %>%\n  count(word, sort = T)\n```\n\nLet's plot the frequencies of the top 20 words:\n\n```{r}\ntidy_text %>%\n  count(word, sort = T) %>%\n  top_n(20) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL) + \n  my_theme()\n```\n\n## Sentiments\n\nWe can also do some basic sentiment analysis to see how positive or negative word usage was.\n\nFor example, we can ask: How negative is Jenny?\n\n```{r}\ntidy_text %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(sentiment, sort = T)\n```\n\nWe can also create an \"index\" variable to chunk the text. In this case, since the texts are across time, we can get a sense of changes in word usage over time.\n\nDoes her negativity change over time?\n\n```{r}\ntidy_text %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(sentiment, index = line%/%100, sort = T)\n```\n\nWe can also plot that:\n\n```{r}\np <- tidy_text %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(sentiment, index = line%/%100, sort = T) %>%\n  ggplot(aes(x = index, y = n, color = sentiment)) + \n    geom_line() + \n    geom_point() + \n    my_theme()\np\n```\n\nWe see a bifurcation later on that may correspond to the death of her son.\n\nLet's format that a bit more:\n\n```{r}\np + \n  scale_color_manual(values = c(\"grey40\", \"goldenrod\")) + \n  scale_x_continuous(limits = c(0,18), breaks = seq(0,15,5)) + \n  annotate(\"label\"\n           , label = \"negative\"\n           , y = 32\n           , x = 15.5\n           , hjust = 0\n           , fill = \"grey40\"\n           , color = \"white\") + \n  annotate(\"label\"\n           , label = \"positive\"\n           , y = 13\n           , x = 15.5\n           , hjust = 0\n           , fill = \"goldenrod\")  +\n  labs(x = \"Chunk\", y = \"Count\") + \n  theme(legend.position = \"none\")\n```\n\nWe can also look at the most common negative and positive words:\n\n```{r}\ntidy_text %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(sentiment, word, sort = T) %>%\n  group_by(sentiment) %>%\n  top_n(10)\n```\n\nand plot those:\n\n```{r}\np <- tidy_text %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(sentiment, word, sort = T) %>%\n  group_by(sentiment) %>%\n  top_n(10) %>%\n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(x = n, y = word, fill = sentiment)) +\n  geom_col() +\n  labs(y = NULL) + \n  facet_wrap(~sentiment, scales = \"free_y\") +\n  my_theme()\np\n```\n\nSome small aesthetic touches:\n\n```{r}\np + \n  scale_fill_manual(values = c(\"grey40\", \"goldenrod\")) + \n  theme(legend.position = \"none\")\n```\n\n## Word Clouds\n\nWord clouds are another way to depict word usage / frequency. Rather than having an axis like our bar graph, it uses relative text size to communicate the same information.\n\n```{r}\ntidy_text %>%\n  count(word) %>%\n  with(wordcloud(\n    word\n    , n\n    , max.words = 100)\n    )\n```\n\nWe can also use custom color palettes:\n\n```{r}\npal <- brewer.pal(6,\"Dark2\")\ntidy_text %>%\n  count(word) %>%\n  with(wordcloud(\n    word\n    , n\n    , max.words = 100\n    , colors = pal)\n    )\n```\n\nAnd split by positive v negative words:\n\n```{r}\npar(mar = c(0, 0, 0, 0), mfrow = c(1,2))\ntidy_text %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(sentiment, word, sort = T) %>%\n  filter(sentiment == \"negative\") %>%\n  with(wordcloud(\n    word\n    , n\n    , max.words = 100\n    , colors = \"grey40\")\n    )\ntitle(\"Negative\", line = -2)\n\ntidy_text %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(sentiment, word, sort = T) %>%\n  filter(sentiment == \"positive\") %>%\n  with(wordcloud(\n    word\n    , n\n    , max.words = 100\n    , colors = \"goldenrod\")\n    )\ntitle(\"Positive\", line = -2)\n```\n\n# `ggplot2` hacks\n\n## Data\n\n-   Data cleaning is often the hardest, most time consuming part of our research flow\n-   Whether we are cleaning raw data, or cleaning data that come out of a model object, we have to be able to wrangle it to the shape we need for whatever program we're using\\\n-   Other than lots of tools in your toolbox for reshaping (see [Week 1](https://emoriebeck.github.io/psc290-data-viz-2022/01-week1-slides-code.html#/title-slide)), the biggest data cleaning hack I have has nothing to do with cleaning, per se\n\n### Two Key Rules of Data Cleaning:\n\n-   Specifically, data cleaning requires two things:\n    -   You have to know what the output you want is (in our case, plots)\n    -   You have know how what the data need to look like to produce that\n\n### Example: Corrlelograms and Heat Maps\n\n-   Let's consider an example, going back to when we wanted to make correlelograms / heat maps.\n-   Here's the plot we wanted to create:\n\n```{r}\nload(url(\"https://github.com/emoriebeck/psc290-data-viz-2022/blob/main/04-week4-associations/04-data/week4-data.RData?raw=true\"))\n\nr_data <- pred_data %>%\n  select(study, p_value, age, gender, SRhealth, smokes, exercise, BMI, education, parEdu, mortality = o_value) %>%\n  mutate_if(is.factor, ~as.numeric(as.character(.))) %>%\n  group_by(study) %>%\n  nest() %>%\n  ungroup() %>%\n  mutate(r = map(data, ~cor(., use = \"pairwise\")))\n\nr_reshape_fun <- function(r){\n  coln <- colnames(r)\n  # remove lower tri and diagonal\n  r[lower.tri(r, diag = T)] <- NA\n  r %>% data.frame() %>%\n    rownames_to_column(\"V1\") %>%\n    pivot_longer(\n      cols = -V1\n      , values_to = \"r\"\n      , names_to = \"V2\"\n    ) %>%\n    mutate_at(vars(V1, V2), ~factor(., coln))\n}\n\nr_data <- r_data %>%\n  mutate(r_long = map(r, r_reshape_fun))\n\nhmp <- r_data$r_long[[1]] %>%\n  ggplot(aes(x = V1, y = V2, fill = r)) + \n  geom_raster() + \n  geom_text(aes(label = round(r, 2))) + \n  scale_fill_gradient2(limits = c(-1,1)\n    , breaks = c(-1, -.5, 0, .5, 1)\n    , low = \"blue\", high = \"red\"\n    , mid = \"white\", na.value = \"white\") + \n  labs(\n    x = NULL\n    , y = NULL\n    , fill = \"Zero-Order Correlation\"\n    , title = \"Zero-Order Correlations Among Variables\"\n    , subtitle = \"Sample 1\"\n    ) + \n  theme_classic() + \n  theme(\n    legend.position = \"bottom\"\n    , axis.text = element_text(face = \"bold\")\n    , axis.text.x = element_text(angle = 45, hjust = 1)\n    , plot.title = element_text(face = \"bold\", hjust = .5)\n    , plot.subtitle = element_text(face = \"italic\", hjust = .5)\n    , panel.background = element_rect(color = \"black\", size = 1)\n  )\n```\n\n-   This seems like it should be straightforward because we're taking a correlation matrix and... visualizing it as a matrix\\\n-   But `ggplot2` doesn't communicate with correlation matrices because they are in **wide format**\n-   So we need to figure out how to make the correlation matrix long format in ways that gives us:\n    -   Variables on the x-axis\n    -   Variables on the y-axis\n    -   Correlations for fill\n    -   Correlations (rounded) for text\n    -   **no double dipping on values**\n\n```{r}\nhmp\n```\n\n-   If you remember nothing else from this course, please remember this:\n    -   **AESTHETIC MAPPINGS CORRESPOND TO COLUMNS IN THE DATA FRAME YOU ARE PLOTTING**\n-   So if want all of the above we need the following columns:\n    -   V1 (x)\n    -   V2 (y)\n    -   r (fill, text)\n-   But what do we currently have?\n    -   A p\\*p correlation matrix\n    -   `ggplot2` wants a data frame\n-   Where are the variable labels (our eventual V1 \\[x\\] and V2 \\[y\\])?\n    -   Column names (`colnames()`) and row names (`rownames()`)\n-   Where are our correlations?\n    -   In wide format (unindexed by explicit columns)\n\n```{r}\nr_data$r[[1]]\n```\n\n-   As a reminder, here's our criteria for what we want our data to look like to plot:\n\n    -   V1 (x)\n    -   V2 (y)\n    -   r (fill, text)\n    -   **no double dipping on values**\n    -   Must be a data frame\n\n-   But these aren't in the right order\n\n-   It should be these steps:\n\n    -   **no double dipping on values**\n    -   Must be a data frame\n    -   V1 (x)\n    -   V2 (y); r (fill, text)\n\n-   Last but, BUT we have also been learning lots about `ggplot2` default behavior, and one of those things is that it will treat columns of `class()` `character` as something that should be ordered alphabetically via `scale_[map]_discrete()`\n\n    -   If we don't want it to, we need to make it a `factor` with `levels` and/or `labels` we provide\n    -   For a heat map / correlelogram, it is *imperative* that this order is the same order you gave `cor()` with the raw data.\n\n-   You can see that order by looking at the row and column names:\n\n```{r}\nr_data$r[[1]]\n```\n\n#### Get variable order from correlation matrix\n\n```{r}\nr <- r_data$r[[1]]\ncoln <- colnames(r)\ncoln\n```\n\n### No double dipping on values\n\n```{r}\nr <- r_data$r[[1]]\ncoln <- colnames(r)\nr[lower.tri(r, diag = T)] <- NA\nr\n```\n\n#### Must be a data frame\n\n```{r}\nr <- r_data$r[[1]]\ncoln <- colnames(r)\nr[lower.tri(r, diag = T)] <- NA\nr %>% data.frame()\n```\n\n#### V1 (x)\n\n```{r}\nr <- r_data$r[[1]]\ncoln <- colnames(r)\nr[lower.tri(r, diag = T)] <- NA\nr %>% data.frame() %>%\n  rownames_to_column(\"V1\")\n```\n\n#### V2 (y); r (fill, text)\n\n```{r}\nr <- r_data$r[[1]]\ncoln <- colnames(r)\nr[lower.tri(r, diag = T)] <- NA\nr %>% data.frame() %>%\n  rownames_to_column(\"V1\") %>%\n  pivot_longer(\n    cols = -V1\n    , values_to = \"r\"\n    , names_to = \"V2\"\n  )\n```\n\n#### Preserve variable order through factors\n\n```{r}\nr <- r_data$r[[1]]\ncoln <- colnames(r)\nr[lower.tri(r, diag = T)] <- NA\nr %>% data.frame() %>%\n  rownames_to_column(\"V1\") %>%\n  pivot_longer(\n    cols = -V1\n    , values_to = \"r\"\n    , names_to = \"V2\"\n  ) %>%\n  mutate(V1 = factor(V1, levels = rev(coln))\n         , V2 = factor(V2, levels = coln))\n```\n\n### Final Words\n\n-   Data cleaning is anxiety-provoking for lots of really valid reasons\\\n-   You probably outline your writing, so why not outline your data cleaning? It's writing, too\n-   Start by figuring out three things:\n    -   What do you data look like now\n    -   What's your final product (table, visualization, etc.)\n    -   What do your data need to look like to be able to feed into that final product?\n-   Then, start filling out the middle:\n    -   How you do get to that end point?\n-   Don't be afraid to use cheat sheets!\n    -   `tidyr`\n    -   `dplyr`\n    -   `plyr`\n    -   `purrr`\n-   And also don't be afraid to ask questions!\n\n## Axes\n\n### Axes: Bar Charts\n\n-   Remember when we talked about bar charts?\n-   When we measure things, we are careful about scales, wording, etc.\n-   But when we plot our measures, we sometimes fail to give it the same thoughtfulness\n-   Our axes should be representative of our measures!\n\n```{r}\nload(url(\"https://github.com/emoriebeck/psc290-data-viz-2022/raw/main/05-week5-time-series/01-data/ipcs_data.RData\"))\n```\n\nFirst, let's wrangle the data to long form:\n\n```{r}\nipcs_long <- ipcs_data %>%\n  filter(SID == \"02\") %>%\n  select(SID:purposeful) %>%\n  pivot_longer(\n    cols = c(-SID, -Full_Date)\n    , values_to = \"value\"\n    , names_to = \"var\"\n    , values_drop_na = T\n  ) %>%\n  mutate(valence = ifelse(var %in% c(\"afraid\", \"angry\", \"guilty\"), \"Negative\", \"Positive\"))\nipcs_long\n```\n\nNow let's get the means and SD's and plot them:\n\n```{r}\nipcs_long %>%\n  group_by(var, valence) %>%\n  summarize_at(vars(value), lst(mean, sd)) %>%\n  ungroup() %>%\n  ggplot(aes(x = var, y = mean, fill = valence)) + \n    geom_bar(\n      stat = \"identity\"\n      , position = \"dodge\"\n      ) + \n    geom_errorbar(\n      aes(ymin = mean - sd, ymax = mean + sd)\n      , width = .1\n      ) +\n    facet_grid(~valence, scales = \"free_x\", space = \"free_x\") + \n    my_theme()\n```\n\n-   But our scale is 1-5, so it doesn't make much sense to have 0 as the bottom of our y-axis\\\n-   But `ggplot2` won't just let us change the scale minumum, so we have to hack it to allow us to to be able to show the first point scale\n-   To do this, we simply have to subtract 1 from the means, which will effectively make the scale 0-4\n-   Then, we can \"undo\" this by changign the y-axis `labels`\n\n```{r}\nipcs_long %>%\n  group_by(var, valence) %>%\n  summarize_at(vars(value), lst(mean, sd)) %>%\n  ungroup() %>%\n  ggplot(aes(x = var, y = mean - 1, fill = valence)) + \n    geom_bar(\n      stat = \"identity\"\n      , position = \"dodge\"\n      ) + \n    geom_errorbar(\n      aes(ymin = mean - 1 - sd, ymax = mean - 1 + sd)\n      , width = .1\n      ) +\n    scale_y_continuous(limits = c(0,4), breaks = seq(0,4,1), labels = 1:5) + \n    facet_grid(~valence, scales = \"free_x\", space = \"free_x\") + \n    my_theme()\n```\n\nLet's add the raw data in, too!\n\n```{r}\np <- ipcs_long %>%\n  group_by(var, valence) %>%\n  summarize_at(vars(value), lst(mean, sd)) %>%\n  ungroup() %>%\n  ggplot(aes(x = var, y = mean - 1, fill = valence)) + \n    geom_bar(\n      stat = \"identity\"\n      , position = \"dodge\"\n      ) + \n    geom_jitter(\n      data = ipcs_long\n      , aes(y = value - 1, fill = valence)\n      , color = \"black\"\n      , shape = 21\n      , alpha = .5\n      , width = .2\n      , height = .1\n    ) + \n    geom_errorbar(\n      aes(ymin = mean - 1 - sd, ymax = mean - 1 + sd)\n      , width = .1\n      ) +\n    scale_y_continuous(limits = c(-.1,4), breaks = seq(0,4,1), labels = 1:5) +\n    facet_grid(~valence, scales = \"free_x\", space = \"free_x\") + \n    my_theme()\np\n```\n\nAnd do soem small aesthetic touches.\n\n```{r}\np + \n  labs(\n    x = NULL\n    , y = \"Mean Rating (1-5) + SD\"\n  ) + \n  theme(\n    legend.position = \"none\"\n    , axis.text.x = element_text(angle = 45, hjust = 1)\n    )\n```\n\n### Axes: Another Example\n\n-   Here's a plot I was making for a grant last week, demonstrating different mean-level patterns of a behavior across situations from 1 to n.\n-   Note the ... in the axis, which is normal notation to indicate some unknown quantity.\n-   How would we create this?\n\n```{r, echo = F}\ntibble(\n  p = as.character(rep(1, 4))\n  , x = 1:4\n  , y = c(1, 2, 4, 3)\n  ) %>%\n  ggplot(aes(x = x, y = y, group = p)) +\n    geom_line(size = 1, color = \"#8cdbbe\") + \n    geom_point(size = 2.5, color = \"black\", shape = \"square\") + \n    scale_x_continuous(limits = c(.9, 4.1), breaks = c(1, 2, 3, 3.5, 4), labels = c(\"S1\", \"S2\", \"S3\", \"...\", \"Sn\")) + \n    labs(x = \"Situation\", y = \"Mean Response\", title = \"Intraindividual Variability\", subtitle = \"Person 1\") + \n    theme_classic() + \n    theme(axis.ticks.x = element_line(size = c(.5, .5, .5, 0, .5))\n          , axis.text = element_text(face = \"bold\", size = rel(1), color = \"black\")\n          , axis.title = element_text(face = \"bold\", size = rel(1))\n          , plot.title = element_text(face = \"bold\", size = rel(1.2), hjust = .5)\n          , plot.subtitle = element_text(face = \"italic\", size = rel(1.1), hjust = .5)\n          , plot.background = element_rect(color = \"white\", fill = \"white\"))\n```\n\nHere's the data:\n\n```{r}\ntibble(\n  p = as.character(rep(1, 4))\n  , x = paste0(\"S\", c(1,2,3,\"p\"))\n  , y = c(1, 2, 4, 3)\n  ) \n```\n\nLet's add the core ggplot code:\n\n```{r}\ntibble(\n  p = as.character(rep(1, 4))\n  , x = paste0(\"S\", c(1,2,3,\"p\"))\n  , y = c(1, 2, 4, 3)\n  ) %>%\n  ggplot(aes(x = x, y = y, group = p))\n```\n\nAnd our `geoms`, `labs`, and `theme`:\n\n```{r}\ntibble(\n  p = as.character(rep(1, 4))\n  , x = paste0(\"S\", c(1,2,3,\"p\"))\n  , y = c(1, 2, 4, 3)\n  ) %>%\n  ggplot(aes(x = x, y = y, group = p)) + \n  geom_line(size = 1, color = \"#8cdbbe\") + \n    geom_point(size = 2.5, color = \"black\", shape = \"square\") + \n    labs(x = \"Situation\", y = \"Mean Response\", title = \"Intraindividual Variability\", subtitle = \"Person 1\") + \n    my_theme()\n```\n\nBut how do we add the ...?\n\nLet's switch to a continuous scale, then we can use `labels` to add it!\n\n```{r}\ntibble(\n  p = as.character(rep(1, 4))\n  , x = paste0(\"S\", c(1,2,3,\"p\"))\n  , x2 = 1:4\n  , y = c(1, 2, 4, 3)\n  ) %>%\n  ggplot(aes(x = x2, y = y, group = p)) + \n  geom_line(size = 1, color = \"#8cdbbe\") + \n    geom_point(size = 2.5, color = \"black\", shape = \"square\") + \n    labs(x = \"Situation\", y = \"Mean Response\", title = \"Intraindividual Variability\", subtitle = \"Person 1\") + \n    my_theme()\n```\n\nNow that our scale is continuous, we can use `scale_x_continuous()` to set breaks and labels where we want them and saying what we want:\n\n```{r}\ntibble(\n  p = as.character(rep(1, 4))\n  , x = paste0(\"S\", c(1,2,3,\"p\"))\n  , x2 = 1:4\n  , y = c(1, 2, 4, 3)\n  ) %>%\n  ggplot(aes(x = x2, y = y, group = p)) + \n  geom_line(size = 1, color = \"#8cdbbe\") + \n    geom_point(size = 2.5, color = \"black\", shape = \"square\") + \n    scale_x_continuous(\n      limits = c(.9, 4.1)\n      , breaks = c(1,2,3,3.5,4)\n      , labels = c(\"S1\", \"S2\", \"S3\", \"...\", \"S4\")\n      ) + \n    labs(x = \"Situation\", y = \"Mean Response\", title = \"Intraindividual Variability\", subtitle = \"Person 1\") + \n    my_theme()\n```\n\nAlmost there, but we don't want the tick mark at \"...\"\n\nWe can actually supply a vector of length `breaks` to `axis.ticks.x` specifying the `size` of the ticks!\n\n```{r}\ntibble(\n  p = as.character(rep(1, 4))\n  , x = paste0(\"S\", c(1,2,3,\"p\"))\n  , x2 = 1:4\n  , y = c(1, 2, 4, 3)\n  ) %>%\n  ggplot(aes(x = x2, y = y, group = p)) + \n  geom_line(size = 1, color = \"#8cdbbe\") + \n    geom_point(size = 2.5, color = \"black\", shape = \"square\") + \n    scale_x_continuous(\n      limits = c(.9, 4.1)\n      , breaks = c(1,2,3,3.5,4)\n      , labels = c(\"S1\", \"S2\", \"S3\", \"...\", \"Sn\")\n      ) + \n    labs(x = \"Situation\", y = \"Mean Response\", title = \"Intraindividual Variability\", subtitle = \"Person 1\") + \n    my_theme() + \n    theme(axis.ticks.x = element_line(color = c(rep(.5, 3), 0, .5)))\n```\n\n## Scales\n\n-   `coord_cartesian()`: the default and what you'll use most of the time\n-   `coord_polar()`: remember Trig and Calculus?\n-   `coord_quickmap()`: sets you up to plot maps\n-   `coord_trans()`: apply transformations to coordinate plane\n-   `coord_flip()`: flip `x` and `y`\n\n### `coord_polar()`\n\nHere's some data we'll use\n\n```{r}\nipcs_m <- ipcs_data %>% \n  filter(SID %in% c(216, 211, 174)) %>%\n  select(SID, Full_Date, afraid:purposeful, Adversity:Sociability)\nipcs_m\n```\n\n-   Let's:\n    -   Grab the variable names (`vars`)\n    -   Make the data long (`pivot_longer()`)\n    -   get means and sd's for the participant\n\n```{r}\nvars <- colnames(ipcs_m)[c(-1, -2)]\nipcs_m <- ipcs_m %>%\n  pivot_longer(\n    cols = c(-SID, -Full_Date)\n    , values_to = \"value\"\n    , names_to = \"var\"\n    , values_drop_na = T\n  ) %>%\n  group_by(SID, var) %>%\n  summarize(m = mean(value)\n         , sd = sd(value)) %>%\n  ungroup()\nipcs_m\n```\n\nLet's use the `vars` vector to create a data frame that also gives each variable:\n\n-   A category label\n-   A integer value\n\nThen, we can use the integer value as the `x` aesthetic mapping, which will let us \"hack\" that axis later:\n\n```{r}\nvars <- tibble(\n  var = vars\n  , cat = c(rep(\"Emotion\", 10), rep(\"Situation\", 8))\n  , num = 1:length(vars)\n)\n\nipcs_m <- ipcs_m %>%\n  left_join(vars %>% rename(var2 = num)) \n\np <- ipcs_m %>%\n  ggplot(aes(x = var2, y = m, fill = cat)) + \n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    my_theme() +\n    facet_wrap(~SID)\np\n```\n\nLet's change the fill colors:\n\n```{r}\np <- p + \n  scale_fill_brewer(palette = \"Set2\")\np\n```\n\nChange the scale to polar:\n\n```{r}\np <- p + \n  coord_polar()\np\n```\n\n-   Now, let's:\n    -   set the angle we want the text labels on\n    -   add the text labels\\\n    -   change the y scale to add some white space in the middle (kind of like a donut chart)\n\n```{r}\nangle <- 90 - 360 * (ipcs_m$var2-0.5) / nrow(vars)\n\np <- p + \n  geom_text(\n    aes(label = var, y = m + .5)\n    , angle = angle\n    , hjust = 0\n    , size = 3\n    , alpha = .6\n    ) + \n  scale_y_continuous(limits = c(-2, 6.5))\np\n```\n\nAnd do some aesthetic stuff:\n\n```{r}\np <- p + \n  labs(\n    fill = \"Feature Category\"\n    , title = \"Relative Differences in Intraindividual Means\"\n    , subtitle = \"Across Emotions and Situation Perceptions\"\n    ) + \n  theme(\n    axis.line = element_blank()\n    , axis.text = element_blank()\n    , axis.ticks = element_blank()\n    , axis.title = element_blank()\n    , panel.background = element_rect(color = \"black\", fill = NA, size = 1)\n  ) \np\n```\n\n## Points\n\n-   You can make points any text character. Here, we'll change points representing men to \"M\" and women to \"W\"\n\n```{r}\npred_data %>%\n  filter(study == \"Study1\") %>%\n  ggplot(aes(x = p_value, y = SRhealth)) + \n    geom_point(aes(shape = gender, color = gender), size = 3, alpha = .75) + \n    scale_shape_manual(values = c(\"M\", \"W\")) + \n    scale_color_manual(values = c(\"blue\", \"red\")) + \n    my_theme()\n```\n\n## Annotations\n\n-   Annotations are a great way to hack because they *don't require data frame input*\n    -   [`\"text\"`](https://emoriebeck.github.io/psc290-data-viz-2022/07-week7-slides-code.html#/example-setup-forest-plot-table-6)\n    -   [`\"label\"`](https://emoriebeck.github.io/psc290-data-viz-2022/05-week5-slides-code.html#/multivariate-time-series-3)\n    -   [`\"rect\"`](https://emoriebeck.github.io/psc290-data-viz-2022/07-week7-slides-code.html#/example-setup-forest-plot-9) and [here](https://emoriebeck.github.io/psc290-data-viz-2022/05-week5-slides-code.html#/multivariate-time-series-7)\n    -   lines\\\n    -   [`\"segment\"`](https://emoriebeck.github.io/psc290-data-viz-2022/06-week6-slides-code.html#/error-bars-3)\n    -   [`\"arrows\"`](https://emoriebeck.github.io/psc290-data-viz-2022/06-week6-slides-code.html#/error-bars-3)\\\n    -   ... and more!\n\n### Text\n\n-   You've already seen lots of example of using `annotate(\"text\", ...)`\n-   But we can also use `annotate(\"text\", label = \"mu\", parse = T)` or `annotate(\"text\", label = expression(mu[i]), parse = T)` to produce math text in our geoms\n\nHere's another figure from a grant I'm working on that uses several of the features we've been discussing: Specifically, notice the line that has `label = \"mu\", parse = T`, which creates the Greek letter on the figure.\n\n```{r}\nset.seed(11)\n\ndist_df = tibble(\n  dist = dist_normal(3,0.75),\n  dist_name = format(dist)\n)\n\ndist_df %>%\n  ggplot(aes(y = 1, xdist = dist)) +\n  stat_slab(fill = \"#8cdbbe\") + \n  annotate(\"point\", x = 3, y = 1, size = 3) +\n  annotate(\"text\", label = \"mu\", x = 3, y = .92, parse = T, size = 8) + \n  annotate(\"text\", label = \"people\", x = 2, y = .95) + \n  annotate(\"segment\", arrow = arrow(type = \"closed\", length=unit(2, \"mm\")), size = 1, x = 2.8, xend = 1.2, y = .98, yend = .98) + \n  annotate(\"text\", label = \"people\", x = 4, y = .95) + \n  annotate(\"segment\", arrow = arrow(type = \"closed\", length=unit(2, \"mm\")), size = 1, x = 3.2, xend = 4.8, y = .98, yend = .98) + \n  labs(title = \"Between-Person Differences\") + \n  theme_void()+ \n  theme(plot.title = element_text(face = \"bold\", size = rel(1.2), hjust = .5))\n\n```\n\nBut if instead we wanted to emphasize *one person's* mean and distribution of their psychological states rather than population-level differences, then we want to show $\\mu_i$, not $\\mu$. If we want it to have a subscript, we can use `expression()`.\n\n```{r}\ndist_df %>%\n  ggplot(aes(y = 1, xdist = dist)) +\n  stat_dots(quantiles = 300, fill = \"#b1c9f2\") + \n  stat_slab(fill = NA, color = \"cornflowerblue\") + \n  annotate(\"point\", x = 3, y = 1, size = 3) +\n  annotate(\"text\", label = expression(mu[i]), x = 3, y = .92, parse = T, size = 8) + \n  annotate(\"segment\", arrow = arrow(type = \"closed\", length=unit(2, \"mm\")), size = 1, x = 1.2, xend = 2, y = 1.4, yend = 1.16) +\n  annotate(\"text\", label = expression(Occasion[i]), x = 1.2, y = 1.425) +\n  labs(title = \"Within-Person Variability\") + \n  theme_void() + \n  theme(plot.title = element_text(face = \"bold\", size = rel(1.2), hjust = .5)\n        , plot.background = element_rect(fill = \"white\"))\n```\n\n## Legends\n\n-   There are several ways to control legends:\n    -   use `theme(legend.position = [arg])` to change its position\n    -   use `labs([mappings] = \"[titles]\")` to control legend titles\\\n    -   use `guides()` to do about everything else\n\n\\###`theme()`\n\n-   `legend.position` takes two kinds of arguments\n    -   text: `\"none\"`, `\"left\"`, `\"right\"` (default), `\"bottom\"`, `\"top\"`\n    -   vector: x and y position (e.g. `c(1,1)`)\n\n```{r}\nhmp + \n  theme(legend.position = \"right\")\n```\n\n-   `legend.position` takes two kinds of arguments\n    -   text: `\"none\"`, `\"left\"`, `\"right\"` (default), `\"bottom\"`, `\"top\"`\n    -   vector: x and y position (e.g. `c(1,1)`)\n\n```{r}\nhmp + \n  theme(legend.position = c(.8, .35))\n```\n\n### `labs`\n\n-   I won't spend too much time here. We've seen this a lot\\\n-   Say that you set `color` and `fill` equal to variable V1\n-   Unless you specify differently, that will be the axis title\n-   You can change this using `labs(fill = \"My Title\", color = \"My Title)`\n-   But make sure you\n    -   Set both\n    -   Make the labels the same or they will not be combined into a single legend\n\n### `guides()`\n\n-   `theme()` lets you control the position of the legend and how it appears\n-   `labs()` lets you control its titles\\\n-   `scale_[map]_[type]` lets you control limits, breaks, and labels\n-   `guides()` lets your control individual legend components\n\nRemember correlelograms? Do we need the size legend?\n\n`{r. echo = F} p <- r_data$r_long[[1]] %>%   ggplot(aes(x = V1, y = V2, fill = r, size = abs(r))) +    geom_point(shape = 21) +    scale_fill_gradient2(limits = c(-1,1)     , breaks = c(-1, -.5, 0, .5, 1)     , low = \"blue\", high = \"red\"     , mid = \"white\", na.value = \"white\") +    scale_size_continuous(range = c(3,14)) +    labs(     x = NULL     , y = NULL     , fill = \"Zero-Order Correlation\"     , title = \"Zero-Order Correlations Among Variables\"     , subtitle = \"Sample 1\"     ) +    theme_classic() +    theme(     legend.position = \"bottom\"     , axis.text = element_text(face = \"bold\")     , axis.text.x = element_text(angle = 45, hjust = 1)     , plot.title = element_text(face = \"bold\", hjust = .5)     , plot.subtitle = element_text(face = \"italic\", hjust = .5)     , panel.background = element_rect(color = \"black\", size = 1)   )`\n\nWe can use `guides()` to remove only the size legend:\n\n```{r}\np + \n  guides(size = \"none\")\n```\n\nAnd for fill, we could change its direction and the number of columns\n\n```{r}\np + \n  guides(\n    size = \"none\"\n    , fill = guide_legend(\n      direction = \"vertical\"\n      , ncol = 2\n      )\n    ) + \n  theme(legend.position = c(.7,.3))\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":true,"code-line-numbers":true,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"highlight-style":"tango","self-contained":true,"output-file":"08-week8-workbook.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","theme":["cosmo","custom-styles.scss"],"title":"Week 8 (Workbook) - Polishing Visualizations","author":"Emorie D Beck","editor_options":{"chunk_output_type":"console"},"code-copy":true,"toc-float":true,"footer":"PSC 290 - Data Visualization","logo":"https://github.com/emoriebeck/psc290-data-viz-2022/raw/main/01-week1-intro/02-code/02-images/ucdavis_logo_blue.png"},"extensions":{"book":{"multiFile":true}}}}}